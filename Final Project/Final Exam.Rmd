---
title: "Final Exam"
author: "Subhalaxmi Rout"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  prettydoc::html_pretty: 
    theme: tactile
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

[YouTube Link:](https://www.youtube.com/watch?v=j2gvoJzzr40&t=7s)


### Problem 1.

Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6.  Then generate a random variable Y that has 10,000 random normal numbers with a mean of $\mu = \sigma = \frac {(N+1)}{2}$. 

Probability.   Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.  Interpret the meaning of all probabilities.

5 points           a.   P(X>x | X>y)		b.  P(X>x, Y>y)		c.  P(X<x | X>y)

5 points.   Investigate whether P(X>x and Y>y)=P(X>x)P(Y>y) by building a table and evaluating the marginal and joint probabilities.

5 points.  Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?


###  $I.$ Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6.  Then generate a random variable Y that has 10,000 random normal numbers with a mean of $\mu = \sigma = \frac {(N+1)}{2}$. 

<span style="color:darkgreen">
**Solution**
</span>

```{r}
set.seed(1)
N <- round(runif(1, 10, 100))
mu <- (N+1)/2
sd <- (N+1)/2
X <- runif(10000,min=1,max=N)
summary(X)
hist(X, col = "steelblue2")
Y <- rnorm(10000,mean = mu,sd = sd)
hist(Y, col = "steelblue2")
summary(Y)
```

Distribution of X showing almost uniform and distibution of Y showing mostly normal. 

### $II.$ Probability.   Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable. 

<span style="color:darkgreen">
**Solution**
</span>

```{r}
x<-median(X)
round(x,2)

y<-quantile(Y,0.25)[[1]]
round(y,2)
```

### Interpret the meaning of all probabilities.

### $(a)$ P(X>x | X>y)

<span style="color:darkgreen">
**Solution**
</span>

$$P(X>x | X>y) = \frac {P(X>x, X>y)}{P(X>y)}$$

```{r}
Pxxandxy <- sum(X>x & X>y)/10000
Pxy <- sum(X>y)/10000
Pxx_xy <- Pxxandxy/Pxy
round(Pxx_xy,2)
```

*The probablity of a random number uniformly ranging from 1 to `r N` being greater than `r round(x,2)` given that it is greater than `r round(y,2)` is `r round(Pxx_xy,2)`*

### $(b)$ P(X>x, Y>y)

<span style="color:darkgreen">
**Solution**
</span>

```{r}
Pxxyy<-sum(X>x & Y>y)/10000
round(Pxxyy,2)
```

*The probablity of a random number uniformly ranging from 1 to `r N` being greater than `r round(x,2)` and  greater than `r round(y,2)` is `r round(Pxxyy,2)`*

### $(c)$ P(X<x | X>y)

<span style="color:darkgreen">
**Solution**
</span>

```{r}
Pxx_and_xy<-sum(X<x & X>y)/10000
round(Pxx_and_xy,2)
```
*The probablity of a random number uniformly ranging from 1 to `r N` being less than `r round(x,2)` given that it is greater than `r round(y,2)` is `r round(Pxx_and_xy,2)`*

### $III.$ Investigate whether P(X>x and Y>y)=P(X>x)P(Y>y) by building a table and evaluating the marginal and joint probabilities.

<span style="color:darkgreen">
**Solution**
</span>

```{r}
library(kableExtra)
Prob_X_x <- (length((X > x)[(X > x) == TRUE]))/(length((X > x)))
Prob_Y_y <- (length((Y > y)[(Y > y) == TRUE]))/(length((Y > y)))

X_x = c(Prob_X_x, Prob_Y_y, Prob_X_x*Prob_Y_y, Prob_X_x*Prob_Y_y)
Y_y = c(Prob_Y_y, Prob_X_x, Prob_X_x*Prob_Y_y, Prob_X_x*Prob_Y_y)

contingency_p <- as.data.frame(cbind(X_x, Y_y))
rownames(contingency_p) <-  c("(X>x)","(Y>y)","(X>x)*(Y>y)","(X>x and Y>y)")

kable(contingency_p) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```

Above table shows, P(X>x and Y>y) and  P(X>x)P(Y>y) are equal.

### $IV.$ Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?

<span style="color:darkgreen">
**Solution**
</span>

We will use above created conitingency table to perform these 2 tests - Ficher’s and Chi Square test. 

Null Hypothesis, $H_0$: X>x and Y>y are independent events

Alternate Hypothesis, $H_A$: Both of these are dependent events

```{r}
fisher.test(contingency_p)
```

```{r}
chisq.test(contingency_p)
```

Both test showing high p-value so, we cannot reject null hyposthesis that X>x and Y>y are independent events.

### Problem 2

**You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.**  https://www.kaggle.com/c/house-prices-advanced-regression-techniques . **I want you to do the following.**

**Descriptive and Inferential Statistics.** Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?

**Linear Algebra and Correlation.**  Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. 

**Calculus-Based Probability & Statistics.**  Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of $\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, $\lambda$)).  Plot a histogram and compare it with a histogram of your original variable.   Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).   Also generate a 95% confidence interval from the empirical data, assuming normality.  Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.

**Modeling.**  Build some type of multiple regression  model and submit your model to the competition board.  Provide your complete model summary and results with analysis.  Report your Kaggle.com user name and score.

<span style="color:darkgreen">
**Solution**
</span>

Download train and test csv file from kaggle and load these in R. The goal of this analysis is predict SalesPrice. We will do below steps.

* Load  data and perform EDA (Exploratory Data Analysis)

    + Descriptive and Inferential Statistics
    + Linear Algebra and Correlation
    + Calculus-Based Probability & Statistics
    
* Clean data
* Build Models
* Select best model to predict salesprice on test dataset

#### Data Load and EDA

Load all necessary libraries.

```{r}
library(ggplot2)
library(kableExtra)
library(visdat)
library(corrplot)
library(MASS)
library(purrr)
```

Train dataset consists of 1460 obsevations and 81 features. Test dataset consists of 1459 obsevations and 80 features.

```{r}
train <- 
  read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/DATA-605/master/Final%20Project/train.csv")
dim(train)
test <- 
  read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/DATA-605/master/Final%20Project/test.csv")
dim(test)
```

```{r}
str(train)
```

#### Descriptive and Inferential Statistics

```{r}
summary(train)
```

**Provide univariate descriptive statistics and appropriate plots for the training data set.**

Pairplot shows the relationship between variables. 

```{r, fig.align='center', fig.width=10}
train_2 <- dplyr::select_if(train, is.numeric) %>% keep(is.numeric)

plot(train_2[1:10], col="blue")
plot(train_2[11:20], col="blue")
plot(train_2[21:30], col="blue")
plot(train_2[31:38], col="blue")
```

Below plot shows the distribution of data.

```{r, fig.align='center', fig.width=10}

ggplot(dplyr::select_if(train, is.numeric) %>% keep(is.numeric) %>% tidyr::gather() , aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_bar(color = "blue", fill = "red")
```

**Provide a scatterplot matrix for at least two of the independent variables and the dependent variable.**

```{r}
plot(train %>%
  dplyr::select(TotalBsmtSF, GrLivArea, YearBuilt, SalePrice), col="blue")
```

**Derive a correlation matrix for any three quantitative variables in the dataset.**

```{r}
correlation_matrix <- train %>%
  dplyr::select(TotalBsmtSF, GrLivArea, YearBuilt) %>%
  cor() %>%
  as.matrix()

kable(correlation_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```

**Test the hypothesis that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.**

```{r}
cor.test(train$TotalBsmtSF, train$SalePrice, method = "pearson", conf.level = 0.8)
cor.test(train$GrLivArea, train$SalePrice, method = "pearson", conf.level = 0.8)
cor.test(train$YearBuilt, train$SalePrice, method = "pearson", conf.level = 0.8)
```

The p-values for all three variables are less than 0.05 and the correlation coefficient between TotalBsmtSF, GrLivArea, YearBuilt with respect to SalePrice is 0.6135806, 0.7086245 , 0.5228973 respectively. The correlation falls within the 80% CI. 

**Would you be worried about familywise error? Why or why not?**

Familywise error:

The familywise error rate is the probability of a coming to at least one false conclusion in a series of hypothesis tests. Familywise error is that of making at least one “type I error”, or a false positive, rejection of a true null. 

In our above 3 cases, the P value is extremely small (is less than 0.05) and rejects null hypothesis. So, I would not be worried about committing a type I error. 

### Linear Algebra and Correlation

**Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.)**

```{r}
library(pracma)

precision_matrix <- pracma::inv(correlation_matrix)
kable(precision_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```

**Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.**

```{r}
kable(round(correlation_matrix %*% precision_matrix)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")

kable(round(precision_matrix %*% correlation_matrix)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```

**Conduct LU decomposition on the matrix.**

Below find out L and U matrix, and as per the LU decomposition, LU = A Here A is the correlation matrix we created above. So, if we multiply L and U above, it should give correlation matrix. 

```{r}
correlation_L <- lu(correlation_matrix)$L
correlation_U <- lu(correlation_matrix)$U

kable(correlation_L %*% correlation_U) %>% 
  kable_styling(bootstrap_options = 
                  c("striped", "hover", "condensed", "responsive"),
                latex_options="scale_down")

correlation_matrix == correlation_L %*% correlation_U
```

### Calculus-Based Probability & Statistics

**Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of $\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, $\lambda$)).  Plot a histogram and compare it with a histogram of your original variable.**

```{r}
hist(train$BsmtUnfSF, main = "Histogram of BsmtUnfSF"
     , col = "darkgreen", 
     xlab = "Unfinished square feet of basement area", breaks = 20)
abline(v = mean(train$BsmtUnfSF), col = "blue", lwd = 2)
abline(v = median(train$BsmtUnfSF), col = "red", lwd = 2)

```

```{r}
lambda <- fitdistr(train$BsmtUnfSF,"exponential")[[1]]
sample <- rexp(1000, lambda)
hist(sample , main="Histogram",xlab="Sample", 
     breaks = 20, col = "steelblue2")
summary(sample)
```

The optimal value for $\lambda$ is `r lambda`.

```{r}
sample.df <- data.frame(length = sample)
Actual.df <- data.frame(length = train$BsmtUnfSF)

sample.df$from <- 'Sample'
Actual.df$from <- 'Actual'

both.df <- rbind(sample.df,Actual.df)

ggplot(both.df, aes(length, fill = from)) + geom_density(alpha = 0.25)
```

The exponential distribution data has a longer tail than the basement square footage data.

**Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).Also generate a 95% confidence interval from the empirical data, assuming normality.  Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.**

Generating the 5th and 95th percentiles

```{r}
qexp(c(0.05,0.95), lambda)
```

Generating 95% confidence level from the data, assuming that the distribution is normal.

```{r}
qnorm(c(0.05, 0.95), mean = mean(train$BsmtUnfSF), sd = sd(train$BsmtUnfSF))
```
Now using the actual data to get 5th and 95th percentile

```{r}
quantile(train$BsmtUnfSF, c(0.05, 0.95))
```

If we model the data as exponentially distributed the 5th percentile is `r qexp(c(0.05,0.95), lambda)[1]`. If we model it as normally distributed the 5th is at `r qnorm(c(0.05, 0.95), mean = mean(train$BsmtUnfSF), sd = sd(train$BsmtUnfSF))[1]`. Actual data distributed the 5th percentile is `r quantile(train$BsmtUnfSF, c(0.05, 0.95))[1]`.If we look at the 95th percentile we have `r qexp(c(0.05,0.95), lambda)[2]` if the data are exponentially distributed, `r qnorm(c(0.05, 0.95), mean = mean(train$BsmtUnfSF), sd = sd(train$BsmtUnfSF))[2]` if it is normally distributed and `r quantile(train$BsmtUnfSF, c(0.05, 0.95))[2]` in actual data. 

### Data Cleaning

To check null values, if available replace with 0.

```{r, fig.align='center'}
visdat::vis_miss(train)
visdat::vis_miss(test)

train$LotFrontage[is.na(train$LotFrontage)] <- median(train$LotFrontage,na.rm=T)
train$GarageYrBlt[is.na(train$GarageYrBlt)] <- median(train$GarageYrBlt,na.rm=T)
train$MasVnrArea[is.na(train$MasVnrArea)] <- median(train$MasVnrArea,na.rm=T)

train$LotFrontage <- as.integer(train$LotFrontage)
train$MasVnrArea <- as.integer(train$MasVnrArea)
train$TotalBsmtSF <- as.integer(train$TotalBsmtSF)
train$GrLivArea <- as.integer(train$GrLivArea)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)

test$LotFrontage[is.na(test$LotFrontage)] <- median(test$LotFrontage,na.rm=T)
test$GarageYrBlt[is.na(test$GarageYrBlt)] <- median(test$GarageYrBlt,na.rm=T)
test$MasVnrArea[is.na(test$MasVnrArea)] <- median(test$MasVnrArea,na.rm=T)
test$LotFrontage[is.na(test$LotFrontage)] <- median(test$LotFrontage,na.rm=T)
test$BsmtFinSF1[is.na(test$BsmtFinSF1)] <- median(test$BsmtFinSF1,na.rm=T)
test$BsmtFullBath[is.na(test$BsmtFullBath)] <- median(test$BsmtFullBath,na.rm=T)
test$BsmtHalfBath[is.na(test$BsmtHalfBath)] <- median(test$BsmtHalfBath,na.rm=T)
test$GarageYrBlt[is.na(test$GarageYrBlt)] <- median(test$GarageYrBlt,na.rm=T)
test$GarageCars[is.na(test$GarageCars)] <- median(test$GarageCars,na.rm=T)
test$BsmtFinSF2[is.na(test$BsmtFinSF2)] <- median(test$BsmtFinSF2,na.rm=T)
test$BsmtUnfSF[is.na(test$BsmtUnfSF)] <- median(test$BsmtUnfSF,na.rm=T)
test$TotalBsmtSF[is.na(test$TotalBsmtSF)] <- median(test$TotalBsmtSF,na.rm=T)
test$GarageArea[is.na(test$GarageArea)] <- median(test$GarageArea,na.rm=T)



test$LotFrontage <- as.integer(test$LotFrontage)
test$MasVnrArea <- as.integer(test$MasVnrArea)
test$TotalBsmtSF <- as.integer(test$TotalBsmtSF)
test$GrLivArea <- as.integer(test$GrLivArea)
test$GarageYrBlt <- as.integer(test$GarageYrBlt)
```

Dropping variables with charecter types.

```{r}
train_4 <- train %>%
  dplyr::select(-Id, -Alley, -MasVnrType, -BsmtQual, -BsmtCond, -BsmtExposure, -BsmtFinType1, -BsmtFinType2, -Electrical, -FireplaceQu, -GarageType,  -GarageFinish, -GarageQual, -GarageCond, -PoolQC, -Fence, -MiscFeature, -MSZoning, -Street, -LotShape, -LandContour, -Utilities, -LotConfig, -LandSlope, -Neighborhood, -Condition1,-Condition2,-BldgType, -HouseStyle, -RoofStyle, -RoofMatl, -Exterior1st, -Exterior2nd, -ExterQual, -ExterCond, -Foundation,  -Heating,-HeatingQC, -CentralAir, -KitchenQual, -Functional, -PavedDrive, -SaleType, -SaleCondition)

test_4 <- test %>%
  dplyr::select(-Id, -Alley, -MasVnrType, -BsmtQual, -BsmtCond, -BsmtExposure, -BsmtFinType1, -BsmtFinType2, -Electrical, -FireplaceQu, -GarageType,  -GarageFinish, -GarageQual, -GarageCond, -PoolQC, -Fence, -MiscFeature, -MSZoning, -Street, -LotShape, -LandContour, -Utilities, -LotConfig, -LandSlope, -Neighborhood, -Condition1,-Condition2,-BldgType, -HouseStyle, -RoofStyle, -RoofMatl, -Exterior1st, -Exterior2nd, -ExterQual, -ExterCond, -Foundation,  -Heating,-HeatingQC, -CentralAir, -KitchenQual, -Functional, -PavedDrive, -SaleType, -SaleCondition)

visdat::vis_miss(train_4)
visdat::vis_miss(test_4)
```

### Model building

Model1 with all variables

```{r}
model1 <- lm(SalePrice ~ ., data = train_4)
summary(model1)

plot(model1)
residual1 <- resid(model1)
hist(residual1, col = "steelblue2")
plot(residual1, col = 'dark red')
```

Model2 with significant variables

```{r}
model2 <- lm(SalePrice ~ MSSubClass + LotArea + OverallQual + OverallCond + 
               YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + BsmtUnfSF +
               X1stFlrSF + X2ndFlrSF + BsmtFullBath + BedroomAbvGr + KitchenAbvGr +
               TotRmsAbvGrd + Fireplaces + GarageYrBlt + GarageCars + WoodDeckSF +
               ScreenPorch, data = train_4)
summary(model2)

plot(model2)
residual2 <- resid(model2)
hist(residual2, col = "steelblue2")
plot(residual2, col = 'dark red')
```

Model3 Random Forest

Random forest is a supervised learning algorithm. The "forest" it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result.

```{r}
library(randomForest)
#pull out Y variable and store into seperate var.
y_train <- train_4$SalePrice
#select all but y variable.
x_train <- subset(train_4, select = -SalePrice)

#run the randomForest model
model3 <- randomForest(x_train, y = y_train, ntree = 500, importance = T)
plot(model3)
```

### Select best model to predict salesprice on test dataset

Select model1 and Model3 to predict the salesPrice on testdata.

```{r}
pred1<-predict(model1, test_4)
#export data for Kaggle
train_5 <- as.data.frame(cbind(test$Id, pred1))
colnames(train_5) <- c("Id", "SalePrice")
write.csv(train_5, file = "Kaggle_Submission1.csv", quote=FALSE, row.names=FALSE)
```

```{r}
pred3<-predict(model3, test_4)
#export data for Kaggle
train_7 <- as.data.frame(cbind(test$Id, pred3))
colnames(train_7) <- c("Id", "SalePrice")
write.csv(train_7, file = "Kaggle_Submission3.csv", quote=FALSE, row.names=FALSE)
```


#### Prediction Submission Scores

Model1 : Linear Regression

Kaggle username: Subhalaxmi Rout

![](/Users/subhalaxmirout/DATA 621/kaggle.png)

Model3 : Random Forest

Kaggle username: Subhalaxmi Rout

![](/Users/subhalaxmirout/DATA 605/kaggle2.png)

#### Future work

Model can be improved using KNN, Decision Tree, XGBoost algorithms or combination of other algorithms. In future  I will work more data cleaning and model building to get optimal prediction. 

### Reference

https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=sample_submission.csv

